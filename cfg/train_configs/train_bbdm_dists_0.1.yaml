# logging:
output_dir: /public_bme2/bme-dgshen/ZhongjianJiang/projects/shanghaitech-uii-mr/runs # ./runs #
project_name: train_bbdm
exp_name: unet_256c_dists_0.1
logging_dir: logs
report_to: wandb
sample_every: 1000 # 1000 # sample every this many steps
sample_steps: 50 # 200 # sample inference steps
resume_from_checkpoint: latest
# model
backbone: bbdm # choices=["bbdm", "nafnet"]
model_channels: 256
pretrained: /public_bme2/bme-dgshen/ZhongjianJiang/projects/shanghaitech-uii-mr/runs/train_bbdm/unet_256c/checkpoints/model_ema.pt # ./runs/<project_name>/<exp_name>/checkpoints/step_xxx.ckpt
# data setup
## dataset
#train_data_path: /mnt/e/deeplearning/data/mri_reconstruction/shanghaitech_uii_mr/deformable_registration_splited_processed/training
#val_data_path: /mnt/e/deeplearning/data/mri_reconstruction/shanghaitech_uii_mr/deformable_registration_splited_processed/validation
train_data_path: /public_bme/data/jiangzhj2023/projects/Data/deformable_registration_splited_processed/training
val_data_path: /public_bme/data/jiangzhj2023/projects/Data/deformable_registration_splited_processed/validation
normalize_type: minmax # ["minmax", "mean"]
use_hq_data: True # True or False

batch_size: 16 # batch size of each GPU, 16 for 80GB MEMORY GPU
resolution: 256 # resolution of input image
# precision
allow_tf32: True
mixed_precision: bf16 # choices=["no", "fp16", "bf16"]
# loss
loss_type: l1_pixel_dists # choices=["l1", "l2", "l1_lpips1", "l1_lpips2", "l1_pixel_dists"]
loss_weight: 0.1 # weight for lpips or pixel_dists loss
# optimization
max_train_steps: 100000 # 100000
checkpointing_steps: 1000 # 1000
checkpoints_total_limit: 10
gradient_accumulation_steps: 2
learning_rate: 1.e-4
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.0
adam_epsilon: 1.e-8
max_grad_norm: 1.0
# seed
seed: 0
# dataloader cpu num workers
num_workers: 8